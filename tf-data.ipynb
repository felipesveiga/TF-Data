{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "97ca09c1",
   "metadata": {
    "papermill": {
     "duration": 0.003897,
     "end_time": "2023-05-13T14:26:38.852819",
     "exception": false,
     "start_time": "2023-05-13T14:26:38.848922",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<h1 style='font-size:40px'> tf.data Pipeline</h1>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            In this notebook I'll practice my skills with the tf.data module solving Exercise 9 from Hands-On Machine Learning with Scikit-Learn and TensorFlow's Chapter 13.\n",
    "        </li>\n",
    "        <li> \n",
    "            The Exercise commands us the following:\n",
    "            <p style='font-style:italic;margin-top:10px'> \n",
    "                Load the Fashion MNIST dataset (introduced in Chapter 10); split\n",
    "it into a training set, a validation set, and a test set; shuffle the\n",
    "training set; and save each dataset to multiple TFRecord files.\n",
    "Each record should be a serialized Example protobuf with two\n",
    "features: the serialized image (use tf.io.serialize_tensor()\n",
    "to serialize each image), and the label. 11 Then use tf.data to create\n",
    "an efficient dataset for each set. Finally, use a Keras model to\n",
    "train these datasets, including a preprocessing layer to standardize\n",
    "each input feature. Try to make the input pipeline as efficient as\n",
    "possible, using TensorBoard to visualize profiling data.\n",
    "            </p>\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca6cd8a3",
   "metadata": {
    "papermill": {
     "duration": 0.002819,
     "end_time": "2023-05-13T14:26:38.858855",
     "exception": false,
     "start_time": "2023-05-13T14:26:38.856036",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<h2 style='font-size:30px'> Data Importing & Splitting</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aeb6ca01",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-13T14:26:38.867109Z",
     "iopub.status.busy": "2023-05-13T14:26:38.866707Z",
     "iopub.status.idle": "2023-05-13T14:26:52.426864Z",
     "shell.execute_reply": "2023-05-13T14:26:52.424907Z"
    },
    "papermill": {
     "duration": 13.567331,
     "end_time": "2023-05-13T14:26:52.429416",
     "exception": false,
     "start_time": "2023-05-13T14:26:38.862085",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
      "29515/29515 [==============================] - 0s 0us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
      "26421880/26421880 [==============================] - 1s 0us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
      "5148/5148 [==============================] - 0s 0us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
      "4422102/4422102 [==============================] - 1s 0us/step\n"
     ]
    }
   ],
   "source": [
    "# Loading the fashion_mnist dataset.\n",
    "from tensorflow.keras.datasets import fashion_mnist\n",
    "from sklearn.model_selection import train_test_split\n",
    "(X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()\n",
    "\n",
    "# Now, generating the validation set with `train_test_split`.\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2bc4004",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-13T14:26:52.442761Z",
     "iopub.status.busy": "2023-05-13T14:26:52.442087Z",
     "iopub.status.idle": "2023-05-13T14:26:52.571507Z",
     "shell.execute_reply": "2023-05-13T14:26:52.570445Z"
    },
    "papermill": {
     "duration": 0.13858,
     "end_time": "2023-05-13T14:26:52.573795",
     "exception": false,
     "start_time": "2023-05-13T14:26:52.435215",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Storing each one of the sets in a tf.data.Dataset object.\n",
    "# The classes store 1000 elements batches. The groups' data will be put into a .tfrecord file.\n",
    "from tensorflow.data import Dataset\n",
    "batch_size = 1000\n",
    "train = Dataset.from_tensor_slices((X_train, y_train)).shuffle(54000).batch(batch_size)\n",
    "val = Dataset.from_tensor_slices((X_val, y_val)).batch(batch_size)\n",
    "test = Dataset.from_tensor_slices((X_test, y_test)).batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "18c9cbf5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-13T14:26:52.587559Z",
     "iopub.status.busy": "2023-05-13T14:26:52.585926Z",
     "iopub.status.idle": "2023-05-13T14:26:52.592835Z",
     "shell.execute_reply": "2023-05-13T14:26:52.591342Z"
    },
    "papermill": {
     "duration": 0.016104,
     "end_time": "2023-05-13T14:26:52.595380",
     "exception": false,
     "start_time": "2023-05-13T14:26:52.579276",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We'll produce 1000 instances .tfrecord files. One corresponding to a bacth created.\n",
    "train_files = len(X_train) // batch_size\n",
    "val_files = len(X_val) // batch_size\n",
    "test_files = len(X_test) // batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79074aa0",
   "metadata": {
    "papermill": {
     "duration": 0.005311,
     "end_time": "2023-05-13T14:26:52.606702",
     "exception": false,
     "start_time": "2023-05-13T14:26:52.601391",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<h2 style='font-size:30px'> .tfrecord's Production</h2>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            TensorFlow records demands data to be stored in protobuf format. We can do so by placing the serialized information in an `Example` class. \n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1dfa6d74",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-13T14:26:52.619452Z",
     "iopub.status.busy": "2023-05-13T14:26:52.619128Z",
     "iopub.status.idle": "2023-05-13T14:26:52.628860Z",
     "shell.execute_reply": "2023-05-13T14:26:52.627787Z"
    },
    "papermill": {
     "duration": 0.018742,
     "end_time": "2023-05-13T14:26:52.631182",
     "exception": false,
     "start_time": "2023-05-13T14:26:52.612440",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tensorflow import Tensor\n",
    "from tensorflow.train import BytesList, Example, Features, Feature, Int64List\n",
    "from tensorflow.io import serialize_tensor, TFRecordWriter\n",
    "\n",
    "def create_example(images:Tensor, targets:Tensor)->str:\n",
    "    '''\n",
    "        Generates a serialized `Example` object holding the pixel intensities and target values from a collection\n",
    "        of MNIST images.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        `images`: A 3-D `tf.Tensor` with the digits pixels. \\n\n",
    "        `targets`: An 1-D `tf.Tensor` with the digits labels.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        An `tf.train.Example` object storing both pixels and target values.\n",
    "    '''\n",
    "    # Serializing the input vectors.\n",
    "    serialized_images, serialized_targets = serialize_tensor(images), serialize_tensor(targets)\n",
    "    example = Example(\n",
    "        features=Features(\n",
    "            feature={\n",
    "            'pixels':Feature(bytes_list=BytesList(value=[serialized_images.numpy()])),\n",
    "            'target':Feature(bytes_list=BytesList(value=[serialized_targets.numpy()]))\n",
    "        }\n",
    "        ))\n",
    "    # Now, converting the `Example` object into a binary string.\n",
    "    return example.SerializeToString()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1f161d36",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-13T14:26:52.644811Z",
     "iopub.status.busy": "2023-05-13T14:26:52.644467Z",
     "iopub.status.idle": "2023-05-13T14:26:52.937443Z",
     "shell.execute_reply": "2023-05-13T14:26:52.936508Z"
    },
    "papermill": {
     "duration": 0.302846,
     "end_time": "2023-05-13T14:26:52.939764",
     "exception": false,
     "start_time": "2023-05-13T14:26:52.636918",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# It is convenient to place all data files in a separate directory.\n",
    "! mkdir mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0e07d95b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-13T14:26:52.953215Z",
     "iopub.status.busy": "2023-05-13T14:26:52.952237Z",
     "iopub.status.idle": "2023-05-13T14:26:53.465568Z",
     "shell.execute_reply": "2023-05-13T14:26:53.464246Z"
    },
    "papermill": {
     "duration": 0.522711,
     "end_time": "2023-05-13T14:26:53.468095",
     "exception": false,
     "start_time": "2023-05-13T14:26:52.945384",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_files(dataset:Dataset, filename:str, directory:str='.')->None:\n",
    "    '''\n",
    "        Creates the .tfrecord's files based on the batches from a provided `dataset`.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        `dataset`: A `tf.data.Dataset` object. \\n\n",
    "        `filename`: A custom name for file identification. \\n\n",
    "        `directory`: A string that indicates the directory where the files are put.\n",
    "    '''\n",
    "    for index, (images, labels) in dataset.enumerate():\n",
    "        file = TFRecordWriter(f'{directory}/{filename}_{index}.tfrecord')\n",
    "        serialized_data = create_example(images, labels)\n",
    "        file.write(serialized_data)\n",
    "\n",
    "# Generating the files.\n",
    "create_files(train, 'train', 'mnist')\n",
    "create_files(val, 'val', 'mnist')\n",
    "create_files(test, 'test', 'mnist')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "598dea84",
   "metadata": {
    "papermill": {
     "duration": 0.005083,
     "end_time": "2023-05-13T14:26:53.478621",
     "exception": false,
     "start_time": "2023-05-13T14:26:53.473538",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<h2 style='font-size:30px'> Data Treatment</h2>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            With the files generated, we can proceed and handle the data importing and its proper treatment.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c87b6c19",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-13T14:26:53.491431Z",
     "iopub.status.busy": "2023-05-13T14:26:53.491105Z",
     "iopub.status.idle": "2023-05-13T14:26:53.547670Z",
     "shell.execute_reply": "2023-05-13T14:26:53.546048Z"
    },
    "papermill": {
     "duration": 0.066178,
     "end_time": "2023-05-13T14:26:53.550215",
     "exception": false,
     "start_time": "2023-05-13T14:26:53.484037",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Reading the data files separately.\n",
    "train_files = Dataset.list_files('mnist/train*')\n",
    "val_files = Dataset.list_files('mnist/val*')\n",
    "test_files = Dataset.list_files('mnist/test*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2ec6669b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-13T14:26:53.563280Z",
     "iopub.status.busy": "2023-05-13T14:26:53.562525Z",
     "iopub.status.idle": "2023-05-13T14:26:53.569811Z",
     "shell.execute_reply": "2023-05-13T14:26:53.569054Z"
    },
    "papermill": {
     "duration": 0.016326,
     "end_time": "2023-05-13T14:26:53.572032",
     "exception": false,
     "start_time": "2023-05-13T14:26:53.555706",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tensorflow import string, uint8\n",
    "from tensorflow.data import AUTOTUNE, TFRecordDataset\n",
    "from tensorflow.io import FixedLenFeature, parse_example, parse_tensor\n",
    "from typing import Iterable, Tuple\n",
    "\n",
    "def preprocess(tfrecord)->Tuple[Tensor, Tensor]:\n",
    "    features = {\n",
    "    'pixels':FixedLenFeature([], string, default_value=''), \n",
    "    'target':FixedLenFeature([], string, default_value='-1')\n",
    "                }\n",
    "    example = parse_example(tfrecord, features) # Returns a dictionary with the serialized images and target values.\n",
    "    pixels, target = parse_tensor(example['pixels'], uint8), parse_tensor(example['target'], uint8)\n",
    "    return pixels, target\n",
    "\n",
    "def read_files(filenames:Iterable[str], shuffle_size=None, num_threads_reading=AUTOTUNE, \n",
    "               num_threads_preprocess=AUTOTUNE):\n",
    "    dataset = TFRecordDataset(filenames, num_parallel_reads=num_threads_reading)\n",
    "    if shuffle_size:\n",
    "        dataset.shuffle(shuffle_size)\n",
    "    return dataset.map(preprocess, num_parallel_calls=num_threads_preprocess).prefetch(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee8b9bcd",
   "metadata": {
    "papermill": {
     "duration": 0.005227,
     "end_time": "2023-05-13T14:26:53.582844",
     "exception": false,
     "start_time": "2023-05-13T14:26:53.577617",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<p style='color:red'> Documentar funções; partir para a preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd6171f9",
   "metadata": {
    "papermill": {
     "duration": 0.00511,
     "end_time": "2023-05-13T14:26:53.593533",
     "exception": false,
     "start_time": "2023-05-13T14:26:53.588423",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<h2 style='font-size:30px'> </h2>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 28.260224,
   "end_time": "2023-05-13T14:26:56.428834",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-05-13T14:26:28.168610",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
