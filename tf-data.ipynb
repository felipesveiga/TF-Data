{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f85b7219",
   "metadata": {
    "papermill": {
     "duration": 0.00918,
     "end_time": "2023-05-31T16:39:39.389684",
     "exception": false,
     "start_time": "2023-05-31T16:39:39.380504",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<h1 style='font-size:40px'> tf.data Pipeline</h1>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            In this notebook I'll practice my skills with the tf.data module solving Exercise 9 from Hands-On Machine Learning with Scikit-Learn and TensorFlow's Chapter 13.\n",
    "        </li>\n",
    "        <li> \n",
    "            The Exercise commands us the following:\n",
    "            <p style='font-style:italic;margin-top:10px'> \n",
    "                Load the Fashion MNIST dataset (introduced in Chapter 10); split\n",
    "it into a training set, a validation set, and a test set; shuffle the\n",
    "training set; and save each dataset to multiple TFRecord files.\n",
    "Each record should be a serialized Example protobuf with two\n",
    "features: the serialized image (use tf.io.serialize_tensor()\n",
    "to serialize each image), and the label. 11 Then use tf.data to create\n",
    "an efficient dataset for each set. Finally, use a Keras model to\n",
    "train these datasets, including a preprocessing layer to standardize\n",
    "each input feature. Try to make the input pipeline as efficient as\n",
    "possible, using TensorBoard to visualize profiling data.\n",
    "            </p>\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc00975d",
   "metadata": {
    "papermill": {
     "duration": 0.007364,
     "end_time": "2023-05-31T16:39:39.405104",
     "exception": false,
     "start_time": "2023-05-31T16:39:39.397740",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<h2 style='font-size:30px'> Data Importing & Splitting</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d058ebe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-31T16:39:39.422965Z",
     "iopub.status.busy": "2023-05-31T16:39:39.422408Z",
     "iopub.status.idle": "2023-05-31T16:39:51.966027Z",
     "shell.execute_reply": "2023-05-31T16:39:51.964460Z"
    },
    "papermill": {
     "duration": 12.556298,
     "end_time": "2023-05-31T16:39:51.969343",
     "exception": false,
     "start_time": "2023-05-31T16:39:39.413045",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
      "29515/29515 [==============================] - 0s 0us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
      "26421880/26421880 [==============================] - 0s 0us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
      "5148/5148 [==============================] - 0s 0us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
      "4422102/4422102 [==============================] - 0s 0us/step\n"
     ]
    }
   ],
   "source": [
    "# Loading the fashion_mnist dataset.\n",
    "from tensorflow.keras.datasets import fashion_mnist\n",
    "from sklearn.model_selection import train_test_split\n",
    "(X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()\n",
    "\n",
    "# Now, generating the validation set with `train_test_split`.\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "662b0118",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-31T16:39:51.991341Z",
     "iopub.status.busy": "2023-05-31T16:39:51.990459Z",
     "iopub.status.idle": "2023-05-31T16:39:52.157067Z",
     "shell.execute_reply": "2023-05-31T16:39:52.155616Z"
    },
    "papermill": {
     "duration": 0.18161,
     "end_time": "2023-05-31T16:39:52.160342",
     "exception": false,
     "start_time": "2023-05-31T16:39:51.978732",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Storing each one of the sets in a tf.data.Dataset object.\n",
    "# The classes store 1000 elements batches. The groups' data will be put into a .tfrecord file.\n",
    "from tensorflow.data import Dataset\n",
    "batch_size = 1000\n",
    "train = Dataset.from_tensor_slices((X_train, y_train)).shuffle(54000).batch(batch_size)\n",
    "val = Dataset.from_tensor_slices((X_val, y_val)).batch(batch_size)\n",
    "test = Dataset.from_tensor_slices((X_test, y_test)).batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c505d7ad",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-31T16:39:52.182373Z",
     "iopub.status.busy": "2023-05-31T16:39:52.181909Z",
     "iopub.status.idle": "2023-05-31T16:39:52.188650Z",
     "shell.execute_reply": "2023-05-31T16:39:52.187274Z"
    },
    "papermill": {
     "duration": 0.02141,
     "end_time": "2023-05-31T16:39:52.191991",
     "exception": false,
     "start_time": "2023-05-31T16:39:52.170581",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We'll produce 1000 instances .tfrecord files. One corresponding to a bacth created.\n",
    "train_files = len(X_train) // batch_size\n",
    "val_files = len(X_val) // batch_size\n",
    "test_files = len(X_test) // batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb9a33a",
   "metadata": {
    "papermill": {
     "duration": 0.009518,
     "end_time": "2023-05-31T16:39:52.211416",
     "exception": false,
     "start_time": "2023-05-31T16:39:52.201898",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<h2 style='font-size:30px'> .tfrecord's Production</h2>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            TensorFlow records demands data to be stored in protobuf format. We can do so by placing the serialized information in an `Example` class. \n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "be8292a7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-31T16:39:52.233849Z",
     "iopub.status.busy": "2023-05-31T16:39:52.233382Z",
     "iopub.status.idle": "2023-05-31T16:39:52.246826Z",
     "shell.execute_reply": "2023-05-31T16:39:52.245534Z"
    },
    "papermill": {
     "duration": 0.028465,
     "end_time": "2023-05-31T16:39:52.249708",
     "exception": false,
     "start_time": "2023-05-31T16:39:52.221243",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tensorflow import Tensor\n",
    "from tensorflow.train import BytesList, Example, Features, Feature, Int64List\n",
    "from tensorflow.io import serialize_tensor, TFRecordWriter\n",
    "\n",
    "def create_example(images:Tensor, targets:Tensor)->str:\n",
    "    '''\n",
    "        Generates a serialized `Example` object holding the pixel intensities and target values from a collection\n",
    "        of MNIST images.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        `images`: A 3-D `tf.Tensor` with the digits pixels. \\n\n",
    "        `targets`: An 1-D `tf.Tensor` with the digits labels.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        An `tf.train.Example` object storing both pixels and target values.\n",
    "    '''\n",
    "    # Serializing the input vectors.\n",
    "    serialized_images, serialized_targets = serialize_tensor(images), serialize_tensor(targets)\n",
    "    example = Example(\n",
    "        features=Features(\n",
    "            feature={\n",
    "            'pixels':Feature(bytes_list=BytesList(value=[serialized_images.numpy()])),\n",
    "            'target':Feature(bytes_list=BytesList(value=[serialized_targets.numpy()]))\n",
    "        }\n",
    "        ))\n",
    "    # Now, converting the `Example` object into a binary string.\n",
    "    return example.SerializeToString()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ff31b619",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-31T16:39:52.270564Z",
     "iopub.status.busy": "2023-05-31T16:39:52.270114Z",
     "iopub.status.idle": "2023-05-31T16:39:53.377956Z",
     "shell.execute_reply": "2023-05-31T16:39:53.376307Z"
    },
    "papermill": {
     "duration": 1.122499,
     "end_time": "2023-05-31T16:39:53.381405",
     "exception": false,
     "start_time": "2023-05-31T16:39:52.258906",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# It is convenient to place all data files in a separate directory.\n",
    "! mkdir mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "20e50cb4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-31T16:39:53.402546Z",
     "iopub.status.busy": "2023-05-31T16:39:53.402063Z",
     "iopub.status.idle": "2023-05-31T16:39:53.946417Z",
     "shell.execute_reply": "2023-05-31T16:39:53.945111Z"
    },
    "papermill": {
     "duration": 0.558568,
     "end_time": "2023-05-31T16:39:53.949377",
     "exception": false,
     "start_time": "2023-05-31T16:39:53.390809",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_files(dataset:Dataset, filename:str, directory:str='.')->None:\n",
    "    '''\n",
    "        Creates the .tfrecord's files based on the batches from a provided `dataset`.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        `dataset`: A `tf.data.Dataset` object. \\n\n",
    "        `filename`: A custom name for file identification. \\n\n",
    "        `directory`: A string that indicates the directory where the files are put.\n",
    "    '''\n",
    "    for index, (images, labels) in dataset.enumerate():\n",
    "        file = TFRecordWriter(f'{directory}/{filename}_{index}.tfrecord')\n",
    "        serialized_data = create_example(images, labels)\n",
    "        file.write(serialized_data)\n",
    "\n",
    "# Generating the files.\n",
    "create_files(train, 'train', 'mnist')\n",
    "create_files(val, 'val', 'mnist')\n",
    "create_files(test, 'test', 'mnist')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b814d4",
   "metadata": {
    "papermill": {
     "duration": 0.009163,
     "end_time": "2023-05-31T16:39:53.967752",
     "exception": false,
     "start_time": "2023-05-31T16:39:53.958589",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<h2 style='font-size:30px'> Data Treatment</h2>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            With the files generated, we can proceed and handle the data importing and its proper treatment.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "48659414",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-31T16:39:53.988344Z",
     "iopub.status.busy": "2023-05-31T16:39:53.987866Z",
     "iopub.status.idle": "2023-05-31T16:39:54.044677Z",
     "shell.execute_reply": "2023-05-31T16:39:54.042385Z"
    },
    "papermill": {
     "duration": 0.070511,
     "end_time": "2023-05-31T16:39:54.047785",
     "exception": false,
     "start_time": "2023-05-31T16:39:53.977274",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Reading the data files separately.\n",
    "train_files = Dataset.list_files('mnist/train*')\n",
    "val_files = Dataset.list_files('mnist/val*')\n",
    "test_files = Dataset.list_files('mnist/test*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "111df938",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-31T16:39:54.069766Z",
     "iopub.status.busy": "2023-05-31T16:39:54.068619Z",
     "iopub.status.idle": "2023-05-31T16:39:54.082165Z",
     "shell.execute_reply": "2023-05-31T16:39:54.080710Z"
    },
    "papermill": {
     "duration": 0.027271,
     "end_time": "2023-05-31T16:39:54.085316",
     "exception": false,
     "start_time": "2023-05-31T16:39:54.058045",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tensorflow import string, uint8\n",
    "from tensorflow.data import AUTOTUNE, TFRecordDataset\n",
    "from tensorflow.io import FixedLenFeature, parse_example, parse_tensor\n",
    "from typing import Optional, Iterable, Tuple\n",
    "\n",
    "def preprocess(tfrecord:Tensor)->Tuple[Tensor, Tensor]:\n",
    "    '''\n",
    "    Reads an encoded protobuf and returns its Tensors in numerical format.\n",
    "    \n",
    "    Parameter\n",
    "    ---------\n",
    "    `tfrecord`: A `tf.Tensor` that stores encoded protobufs.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Two tensors in a tuple. One with the pixel intensities and another containing the target values.\n",
    "    '''\n",
    "    features = {\n",
    "    'pixels':FixedLenFeature([], string, default_value=''), \n",
    "    'target':FixedLenFeature([], string, default_value='-1')\n",
    "                }\n",
    "    example = parse_example(tfrecord, features) # Returns a dictionary with the serialized images and target values.\n",
    "    pixels, target = parse_tensor(example['pixels'], uint8), parse_tensor(example['target'], uint8)\n",
    "    return pixels, target\n",
    "\n",
    "def read_files(filenames:Iterable[str], shuffle_size:Optional[int]=None, num_threads_reading:int=AUTOTUNE, \n",
    "               num_threads_preprocess:int=AUTOTUNE)->Dataset:\n",
    "    '''\n",
    "        Reads the .tfrecord files specified and retrieves a `tf.data.Dataset` object with the processed data.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        `filenames`: The names of the files.\n",
    "        `shuffle_size`: If specified, it shuffles the Dataset using a deck with the specified length.\n",
    "        `num_threads_reading`: The number of threads to use when reading the files.\n",
    "        `num_threads_preprocess`: The number of threads to use when preprocessing the dataset.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        The treated dataset.\n",
    "    '''\n",
    "    dataset = TFRecordDataset(filenames, num_parallel_reads=num_threads_reading)\n",
    "    if shuffle_size:\n",
    "        dataset.shuffle(shuffle_size)\n",
    "    return dataset.map(preprocess, num_parallel_calls=num_threads_preprocess).prefetch(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2777e8c6",
   "metadata": {
    "papermill": {
     "duration": 0.008765,
     "end_time": "2023-05-31T16:39:54.103309",
     "exception": false,
     "start_time": "2023-05-31T16:39:54.094544",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<h2 style='font-size:30px'> Standardization Layer</h2>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            Here, we'll simply code a `tf.layers.Layer` object which fairly does a similar job of the Batch Normalization Layer. The main difference is that $\\mu$ and $\\sigma$ are computed in advance using the `adapt` function.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e897e183",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-31T16:39:54.124187Z",
     "iopub.status.busy": "2023-05-31T16:39:54.123236Z",
     "iopub.status.idle": "2023-05-31T16:39:54.139670Z",
     "shell.execute_reply": "2023-05-31T16:39:54.138503Z"
    },
    "papermill": {
     "duration": 0.030086,
     "end_time": "2023-05-31T16:39:54.142459",
     "exception": false,
     "start_time": "2023-05-31T16:39:54.112373",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# The layer will inherit the properties of the experimental `PreprocessingLayer`.\n",
    "from tensorflow.keras.layers.experimental.preprocessing import PreprocessingLayer\n",
    "from tensorflow.keras.layers import Layer\n",
    "from tensorflow.math import reduce_mean, reduce_std\n",
    "from tensorflow.keras.backend import epsilon\n",
    "from typing import Union\n",
    "\n",
    "class Standardize(PreprocessingLayer):\n",
    "    '''\n",
    "    A `PreprocessingLayer` object that carries out the standardization of a given array accordingly to an \n",
    "    informed axis.\n",
    "    \n",
    "    The necessary stats are computed before training with the `adapt` method. This feature the  class' main \n",
    "    difference from the `BatchNormalization` layer, which computes means and standard deviations on the fly.\n",
    "    '''\n",
    "    def adapt(self, input_data:Union[Dataset, Tensor], axis=0)->None:\n",
    "        '''\n",
    "            Computes means and std's from a provided `tf.Tensor` or `tf.data.Dataset`.\n",
    "            \n",
    "            Paramater\n",
    "            ---------\n",
    "            `input_data`: The array from which the stats are computed.\n",
    "            `axis`: The axis of choice to compute the stats.\n",
    "        '''\n",
    "        self.means = reduce_mean(input_data, axis=axis)\n",
    "        self.stds = reduce_std(input_data, axis=axis)\n",
    "        return self\n",
    "        \n",
    "    def call(self, input_data:Union[Dataset, Tensor])->Union[Dataset, Tensor]:\n",
    "        '''\n",
    "        The method that standardizes the array.\n",
    "        \n",
    "        Parameter\n",
    "        --------\n",
    "        `input_data`: The array in which we perform the standardization.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        The standardized array.\n",
    "        '''\n",
    "        return (input_data - self.means) / (self.stds + epsilon())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e12168",
   "metadata": {
    "papermill": {
     "duration": 0.00879,
     "end_time": "2023-05-31T16:39:54.160330",
     "exception": false,
     "start_time": "2023-05-31T16:39:54.151540",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<h2 style='font-size:30px'> Neural Net Modelling</h2>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            With all the preprocessing stages programmed, we are free to model our Neural Network.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ff467801",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-31T16:39:54.181863Z",
     "iopub.status.busy": "2023-05-31T16:39:54.180791Z",
     "iopub.status.idle": "2023-05-31T16:39:54.389853Z",
     "shell.execute_reply": "2023-05-31T16:39:54.388477Z"
    },
    "papermill": {
     "duration": 0.223533,
     "end_time": "2023-05-31T16:39:54.392968",
     "exception": false,
     "start_time": "2023-05-31T16:39:54.169435",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Reading all the .tfrecords created.\n",
    "train_set = read_files(train_files, shuffle_size=100)\n",
    "val_set = read_files(val_files)\n",
    "test_set = read_files(test_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "32825b73",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-31T16:39:54.413790Z",
     "iopub.status.busy": "2023-05-31T16:39:54.412918Z",
     "iopub.status.idle": "2023-05-31T16:39:54.992924Z",
     "shell.execute_reply": "2023-05-31T16:39:54.991612Z"
    },
    "papermill": {
     "duration": 0.59363,
     "end_time": "2023-05-31T16:39:54.995880",
     "exception": false,
     "start_time": "2023-05-31T16:39:54.402250",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# But before actually fitting the NN, we need to adapt our `Standardize` layer with the training data.\n",
    "from tensorflow import concat, cast, float32\n",
    "train_pixels = list(train_set.map(lambda pixels, target: pixels).take(-1)) # Getting all the training images.\n",
    "train_pixels = cast(concat(train_pixels, axis=0), dtype=float32) # Concatenating the batches so that we end up\n",
    "                                                                # with a single 3-D matrix.\n",
    "\n",
    "# Now, instantiating the `Standardize` class and adapting it to the `train_pixels` data.\n",
    "standardize = Standardize(input_shape=train_pixels.shape[1:]).adapt(train_pixels, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7614f7ac",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-31T16:39:55.016575Z",
     "iopub.status.busy": "2023-05-31T16:39:55.016134Z",
     "iopub.status.idle": "2023-05-31T16:39:55.031326Z",
     "shell.execute_reply": "2023-05-31T16:39:55.030253Z"
    },
    "papermill": {
     "duration": 0.028979,
     "end_time": "2023-05-31T16:39:55.034014",
     "exception": false,
     "start_time": "2023-05-31T16:39:55.005035",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Finally, making our FCNN.\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten, Input\n",
    "from tensorflow.keras.activations import elu, softmax\n",
    "from tensorflow.keras.initializers import GlorotNormal, HeNormal\n",
    "from typing import Callable\n",
    "\n",
    "def _check_length(neurons:Iterable, activations:Iterable)->bool:\n",
    "    '''\n",
    "        Checks whether both provided arrays have equal lengths.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        `neurons`: First Array\n",
    "        `activations`: Second array\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        A boolean indicating the existance of such condition.\n",
    "    '''\n",
    "    if len(neurons) != len(activations):\n",
    "        raise  AttributeError('The `neurons` array must have the same length as `activations`')\n",
    "    return True\n",
    "\n",
    "def my_fcnn(neurons:Iterable[int], activations:Union[Iterable[Callable[[float], float]], Callable[[float], float]],\n",
    "                dropout_ratio:float=None, input_shape:list=[28,28])->Sequential:\n",
    "    # If the user informed a function for `activations`, a list of such callable of the same length as 'hidden_layers'\n",
    "    # is created.\n",
    "    if isinstance(activations, Callable):\n",
    "        activations = [activations] * len(neurons)    \n",
    "    \n",
    "    # `check_length` is of use when 'activations' is given as an array by the user.\n",
    "    _check_length(neurons, activations)\n",
    "    model = Sequential([\n",
    "        Input(shape=input_shape),\n",
    "        Flatten(input_shape=input_shape)\n",
    "    ])\n",
    "    for units, activation in zip(neurons, activations):\n",
    "        model.add(Dense(units, activation=activation, kernel_initializer=HeNormal))\n",
    "        # Adding Dropout if it is the user's wish.\n",
    "        if dropout_ratio:\n",
    "            model.add(Dropout(dropout_ratio))\n",
    "            \n",
    "    # In the end, applying the layer used for inference.\n",
    "    model.add(Dense(units=10, activation=softmax, kernel_initializer=GlorotNormal))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e16c7f0a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-31T16:39:55.058728Z",
     "iopub.status.busy": "2023-05-31T16:39:55.058018Z",
     "iopub.status.idle": "2023-05-31T16:39:55.492782Z",
     "shell.execute_reply": "2023-05-31T16:39:55.491429Z"
    },
    "papermill": {
     "duration": 0.452829,
     "end_time": "2023-05-31T16:39:55.495944",
     "exception": false,
     "start_time": "2023-05-31T16:39:55.043115",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from numpy.random import randint, seed\n",
    "seed(42) # Guaranteeing that the neurons array is the same for all executions. \n",
    "neurons = randint(200, 400, 5)\n",
    "fcnn = my_fcnn(neurons, elu, .5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "53b005b0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-31T16:39:55.516118Z",
     "iopub.status.busy": "2023-05-31T16:39:55.515673Z",
     "iopub.status.idle": "2023-05-31T16:39:55.538624Z",
     "shell.execute_reply": "2023-05-31T16:39:55.537559Z"
    },
    "papermill": {
     "duration": 0.03633,
     "end_time": "2023-05-31T16:39:55.541382",
     "exception": false,
     "start_time": "2023-05-31T16:39:55.505052",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Compiling and fitting the model.\n",
    "from tensorflow.keras.optimizers import Nadam\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "from tensorflow.keras.metrics import Accuracy\n",
    "fcnn.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "675efe95",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-31T16:39:55.562170Z",
     "iopub.status.busy": "2023-05-31T16:39:55.561040Z",
     "iopub.status.idle": "2023-05-31T16:39:55.569284Z",
     "shell.execute_reply": "2023-05-31T16:39:55.568254Z"
    },
    "papermill": {
     "duration": 0.021343,
     "end_time": "2023-05-31T16:39:55.571906",
     "exception": false,
     "start_time": "2023-05-31T16:39:55.550563",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We'll be designing here a LetNet-5 NN. Thus, we'll need some extra components.\n",
    "\n",
    "# The images that feed the Input Layer of such model have 32x32 shape. Since the MNIST digits are 28x28 it is\n",
    "# necessary to add zero-padding to each matrix.\n",
    "from tensorflow.keras.layers import Reshape, ZeroPadding2D\n",
    "\n",
    "# `ZeroPadding2d` demands the inputs to own a dimension for the amount of channels. That's why we are invoking the\n",
    "# `Reshape` layer as well.\n",
    "target_shape = train_pixels.shape[1:]+[1]\n",
    "reshape = Reshape(target_shape=target_shape)\n",
    "\n",
    "# Instantiating the `ZeroPadding2D` layer.\n",
    "zero_padding_2d = ZeroPadding2D(padding=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73fb0255",
   "metadata": {
    "papermill": {
     "duration": 0.008743,
     "end_time": "2023-05-31T16:39:55.589881",
     "exception": false,
     "start_time": "2023-05-31T16:39:55.581138",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<p style='color:red'> Documentar função my_fcnn Compilar e treinar FCNN e depois Lenet5 (usar GPU!)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dfed607",
   "metadata": {
    "papermill": {
     "duration": 0.008676,
     "end_time": "2023-05-31T16:39:55.607552",
     "exception": false,
     "start_time": "2023-05-31T16:39:55.598876",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<h2 style='font-size:30px'> </h2>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 33.534136,
   "end_time": "2023-05-31T16:39:59.181631",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-05-31T16:39:25.647495",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
